# -*- coding: utf-8 -*-
"""BELLMANS EQUATION [ PRO-125]

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P04PvlLXWuqa3PJDJ-9SWsFf0IeqfJhC

## RL Problem to Solve:

The **machine/bot** needs to find a way out to reach the **Goal**(Room Number 5).The machine will start from any room randomly.

<img src="https://s3-whjr-curriculum-uploads.whjr.online/f39c3b14-04c8-4171-8bd4-7693efcdb792.jpg" width= 400>

## RL Components:

* Environment 

<img src="https://s3-whjr-curriculum-uploads.whjr.online/af5a1d77-041a-4bba-be4e-ca26aea97771.png" width= 300>

* Agent

<img src="https://s3-whjr-curriculum-uploads.whjr.online/fda2e793-0302-420c-981b-dbf32ecb1d12.png" width= 200> 

* States

> The agent/machine can be in any of the 6 rooms( 5 rooms + 1 goal room) of the 
lobby. Hence there **6 possible states**.

> <img src="https://s3-whjr-curriculum-uploads.whjr.online/e0f568a6-77af-4b52-873e-8949f388cbb6.jpg" width= 50> 


* Actions

> The agent/machine can choose to move to any of the 6 rooms( 5 rooms + 1 goal room) of the lobby. Hence there also **6 possible actions**.


> <img src="https://s3-whjr-curriculum-uploads.whjr.online/0e684ce5-6454-47bd-bb2d-d3668f7a0370.jpg" width= 300> 


* Rewards

> **Possible Rewards: -1, 0, 100**


> <img src="https://s3-whjr-curriculum-uploads.whjr.online/7f9acae9-73be-41ab-8686-0cd74f4840ad.jpg" width= 300> 


* **Can’t Move**:If there no direct way from one room 
to another, then the reward is -1.

* **Move**: If the machine can move from the current
room(state) to the next room(action)
then the reward is 0.

* **Goal**: If the machine is at/reached the goal, 
then the reward is 100

## Import Modules
"""

import numpy as np
import random

"""## Reward Martix

> Click on the [Reward Matrix](https://whitehatjrcontent.s3.ap-south-1.amazonaws.com/Teacher-Resources/COCOS_Applets/POC/Coding/SimpleQ-RL/rewardsOnly/index.html) link to understand reward allocation.

> <img src="https://s3-whjr-curriculum-uploads.whjr.online/46dc27ea-cc8c-4b7b-959f-88406531a3c3.jpg" width= 400>

* **Can’t Move**:If there no direct way from one room 
to another, then the reward is -1.

* **Move**: If the machine can move from the current
room(state) to the next room(action)
then the reward is 0.

* **Goal**: If the machine is at/reached the goal, 
then the reward is 100.

> <img src="https://s3-whjr-curriculum-uploads.whjr.online/3183f05b-f22f-46b6-96b4-00043c942250.jpg" width= 400>

"""

rewards = np.array([
    [-1, -1, -1, -1,  0,  -1],
    [-1, -1, -1,  0, -1, 100],
    [-1, -1, -1 , 0 ,-1 , -1],
    [-1,  0,  0 ,-1  ,-1 , -1],
    [0, -1, -1 , -1 ,-1 ,100],
    [-1, -1, -1, -1,  0, 100] 
])

"""## Initial State"""

def set_initial_state():
    return np.random.randint(0, 6)

set_initial_state()

"""## Get Action"""

def get_action(current_state, reward_matrix):
    available_action = []
    print("reward_matrix","\n",reward_matrix)    
    for action in enumerate(reward_matrix[current_state]):     
        if action[1]!= -1:            
            available_action.append(action[0]) 
    choose_action = random.choice(available_action)
    print("Current State",current_state)
    print("Random choice of Action from",available_action,"is", choose_action)           
    return choose_action

current_state = 4

"""##Q matrix
**Q-learning** is a reinforcement learning algorithm. Given the current state, it helps to find the best action to be taken by the agent.
Q stands for Quality in Q-learning. Quality represents how useful action is in gaining a reward.
To perform Q-learning, we use **Q-matrix**. It is also in the form of states as rows and actions as columns. Initially, all the elements of the Q- matrix are zeroes.

"""

#Create Q-matrix here
q_matrix=np.zeros([6,6])
print(q_matrix)

gamma = 0.8
def take_action(current_state,reward_matrix,gamma):
    action = get_action(current_state,reward_matrix)

    current_sa_reward = reward_matrix[current_state, action]
    print(current_sa_reward)

    q_sa_value=max(q_matrix[action,])
    print(q_sa_value)

    q_current_state = current_sa_reward + (gamma * q_sa_value)
    print(q_current_state)


    q_matrix[current_state,action]=q_current_state
    print("q_matrix","\n",q_matrix)
    new_state  = action

take_action(current_state,rewards,gamma)

"""## Take Action

[Q-Matrix](https://whitehatjrcontent.s3.ap-south-1.amazonaws.com/Teacher-Resources/COCOS_Applets/POC/Coding/SimpleQ-RL/noEpisodes/index.html)
"""